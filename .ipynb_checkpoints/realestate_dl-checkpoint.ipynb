{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3d199f17-1e45-4570-bf38-33dfcb6ad403",
    "_execution_state": "idle",
    "_uuid": "e0d595e3e7b3574d5a888839255398526fdc6051",
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# A Deep Learning Approach to Real Estate\n",
    "\n",
    "## Background\n",
    "This notebook will take the classic Kaggle Real Estate problem and apply a deep learning approach using modern techniques including dropout, batch normalisation and mini-batch training.\n",
    "\n",
    "## Shoutouts\n",
    "Full credit for this technique goes to Andrew Beam and Max Berggren for their post that inspired this kernel (below). \n",
    "http://maxberggren.se/2017/06/18/deep-learning-vs-xgboost/ \n",
    "\n",
    "https://www.valentinmihov.com/2015/04/17/adult-income-data-set/\n",
    "\n",
    "## To Do\n",
    "- Hyper parameter optimisation in keras  (http://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/, https://github.com/maxpumperla/hyperas)\n",
    "- Ensembling in keras (look at this file in the dir keras-ensemble.py)\n",
    "- Tensorboard (we need to instrument this network to understand if it is converging?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "_cell_guid": "0fce2061-50ec-4eff-b409-56b2d796496a",
    "_execution_state": "idle",
    "_uuid": "5fe42c3ccaf08c3d8f349ec92a28b703665b02d3",
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "#from subprocess import check_output\n",
    "#print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "_cell_guid": "8a58f6a0-62a1-42db-891c-0ad0c82fa794",
    "_execution_state": "idle",
    "_uuid": "6ed4f88d954b1a6d1b8a970e0b791bb308b5b96a",
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "seed = 123456\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "_cell_guid": "bbc685a4-d03b-44b0-869b-75aeebfbd050",
    "_execution_state": "idle",
    "_uuid": "87e987b4b0a44b2bb83d53a02eb8254933624aa7",
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "target_variable = 'saleprice'\n",
    "df = (\n",
    "    pd.read_csv('input/train.csv') # change this to run on kaggle\n",
    "    #pd.read_csv('../input/train.csv')\n",
    "\n",
    "    # Rename columns to lowercase and underscores\n",
    "    .pipe(lambda d: d.rename(columns={\n",
    "        k: v for k, v in zip(\n",
    "            d.columns,\n",
    "            [c.lower().replace(' ', '_') for c in d.columns]\n",
    "        )\n",
    "    }))\n",
    "    # Switch categorical classes to integers\n",
    "    #.assign(**{target_variable: lambda r: r[target_variable].astype('category').cat.codes})\n",
    ")\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Log saleprice\n",
    "df['saleprice'] = np.log(df['saleprice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "_execution_state": "idle",
    "_uuid": "ba804c52e1608bddc7bb1c2ed2607f305cb9cd32",
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import sklearn.preprocessing as preprocessing\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "_execution_state": "idle",
    "_uuid": "2b39c9f8516be0cacd006758d361721925d9076e",
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Encode the categorical features as numbers\n",
    "def number_encode_features(df):\n",
    "    result = df.copy()\n",
    "    encoders = {}\n",
    "    for column in result.columns:\n",
    "        #print(column)\n",
    "        #print(result.dtypes[column])\n",
    "        if result.dtypes[column] == np.object:\n",
    "            encoders[column] = preprocessing.LabelEncoder()\n",
    "            # if there are NaN's in the categorical data fill it with 'None' which becomes another category\n",
    "            result[column] = encoders[column].fit_transform(result[column].fillna(value='None'))\n",
    "    return result, encoders\n",
    "\n",
    "# Calculate the correlation and plot it\n",
    "encoded_data, _ = number_encode_features(df)\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "_execution_state": "idle",
    "_uuid": "2a6aa67fe1034d732eaef425d96a5e3245044b82",
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Data is now in dataframe \"encoded_data\"\n",
    "\n",
    "y = encoded_data[target_variable].values\n",
    "X = (\n",
    "    # Drop target variable\n",
    "    encoded_data.drop(target_variable, axis=1)\n",
    "    # Min-max-scaling (only needed for the DL model)\n",
    "    .pipe(lambda d: (d-d.min())/d.max()).fillna(0)\n",
    "    .as_matrix()\n",
    ")\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "_execution_state": "idle",
    "_uuid": "cdb748020a57a42c0b0126b153b91b7ab8bad78e",
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "test_size = 0.0\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=test_size, random_state=seed\n",
    ")\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "_execution_state": "idle",
    "_uuid": "90b02a0123d8f69b487b83e73824a226907a86f7",
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers.advanced_activations import  LeakyReLU  \n",
    "from keras import optimizers\n",
    "from keras import initializers\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "_execution_state": "busy",
    "_uuid": "a1e0a86af9150363c697c9a685c3d91359424b8a",
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Starting training....\n",
      "Train on 1314 samples, validate on 146 samples\n",
      "Epoch 1/100\n",
      "Epoch 00000: val_loss improved from inf to 138.52853, saving model to best.model\n",
      "3s - loss: 136.5800 - val_loss: 138.5285\n",
      "Epoch 2/100\n",
      "Epoch 00001: val_loss improved from 138.52853 to 131.11693, saving model to best.model\n",
      "0s - loss: 121.8826 - val_loss: 131.1169\n",
      "Epoch 3/100\n",
      "Epoch 00002: val_loss improved from 131.11693 to 123.16247, saving model to best.model\n",
      "0s - loss: 109.7582 - val_loss: 123.1625\n",
      "Epoch 4/100\n",
      "Epoch 00003: val_loss improved from 123.16247 to 115.39986, saving model to best.model\n",
      "0s - loss: 98.3148 - val_loss: 115.3999\n",
      "Epoch 5/100\n",
      "Epoch 00004: val_loss improved from 115.39986 to 108.35216, saving model to best.model\n",
      "0s - loss: 87.6649 - val_loss: 108.3522\n",
      "Epoch 6/100\n",
      "Epoch 00005: val_loss improved from 108.35216 to 102.76442, saving model to best.model\n",
      "0s - loss: 76.6463 - val_loss: 102.7644\n",
      "Epoch 7/100\n",
      "Epoch 00006: val_loss improved from 102.76442 to 97.42483, saving model to best.model\n",
      "0s - loss: 65.7424 - val_loss: 97.4248\n",
      "Epoch 8/100\n",
      "Epoch 00007: val_loss improved from 97.42483 to 91.96534, saving model to best.model\n",
      "0s - loss: 56.6386 - val_loss: 91.9653\n",
      "Epoch 9/100\n",
      "Epoch 00008: val_loss improved from 91.96534 to 86.48408, saving model to best.model\n",
      "0s - loss: 48.4193 - val_loss: 86.4841\n",
      "Epoch 10/100\n",
      "Epoch 00009: val_loss improved from 86.48408 to 82.39249, saving model to best.model\n",
      "0s - loss: 41.5975 - val_loss: 82.3925\n",
      "Epoch 11/100\n",
      "Epoch 00010: val_loss improved from 82.39249 to 75.90308, saving model to best.model\n",
      "0s - loss: 35.1254 - val_loss: 75.9031\n",
      "Epoch 12/100\n",
      "Epoch 00011: val_loss improved from 75.90308 to 67.79515, saving model to best.model\n",
      "0s - loss: 29.1122 - val_loss: 67.7952\n",
      "Epoch 13/100\n",
      "Epoch 00012: val_loss improved from 67.79515 to 62.25356, saving model to best.model\n",
      "0s - loss: 22.8057 - val_loss: 62.2536\n",
      "Epoch 14/100\n",
      "Epoch 00013: val_loss improved from 62.25356 to 56.77419, saving model to best.model\n",
      "0s - loss: 19.8935 - val_loss: 56.7742\n",
      "Epoch 15/100\n",
      "Epoch 00014: val_loss improved from 56.77419 to 51.57827, saving model to best.model\n",
      "0s - loss: 16.0991 - val_loss: 51.5783\n",
      "Epoch 16/100\n",
      "Epoch 00015: val_loss improved from 51.57827 to 46.29143, saving model to best.model\n",
      "0s - loss: 13.3597 - val_loss: 46.2914\n",
      "Epoch 17/100\n",
      "Epoch 00016: val_loss improved from 46.29143 to 41.21964, saving model to best.model\n",
      "0s - loss: 11.2660 - val_loss: 41.2196\n",
      "Epoch 18/100\n",
      "Epoch 00017: val_loss improved from 41.21964 to 34.28037, saving model to best.model\n",
      "0s - loss: 9.1758 - val_loss: 34.2804\n",
      "Epoch 19/100\n",
      "Epoch 00018: val_loss improved from 34.28037 to 29.70639, saving model to best.model\n",
      "0s - loss: 8.4225 - val_loss: 29.7064\n",
      "Epoch 20/100\n",
      "Epoch 00019: val_loss improved from 29.70639 to 27.02485, saving model to best.model\n",
      "0s - loss: 8.5019 - val_loss: 27.0249\n",
      "Epoch 21/100\n",
      "Epoch 00020: val_loss improved from 27.02485 to 24.95205, saving model to best.model\n",
      "0s - loss: 7.7892 - val_loss: 24.9520\n",
      "Epoch 22/100\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 8.0199 - val_loss: 25.7895\n",
      "Epoch 23/100\n",
      "Epoch 00022: val_loss improved from 24.95205 to 21.60432, saving model to best.model\n",
      "0s - loss: 7.8585 - val_loss: 21.6043\n",
      "Epoch 24/100\n",
      "Epoch 00023: val_loss improved from 21.60432 to 19.37132, saving model to best.model\n",
      "0s - loss: 7.7805 - val_loss: 19.3713\n",
      "Epoch 25/100\n",
      "Epoch 00024: val_loss improved from 19.37132 to 15.98958, saving model to best.model\n",
      "0s - loss: 7.5424 - val_loss: 15.9896\n",
      "Epoch 26/100\n",
      "Epoch 00025: val_loss improved from 15.98958 to 12.20676, saving model to best.model\n",
      "0s - loss: 8.1465 - val_loss: 12.2068\n",
      "Epoch 27/100\n",
      "Epoch 00026: val_loss improved from 12.20676 to 11.74409, saving model to best.model\n",
      "0s - loss: 6.9348 - val_loss: 11.7441\n",
      "Epoch 28/100\n",
      "Epoch 00027: val_loss improved from 11.74409 to 11.58699, saving model to best.model\n",
      "0s - loss: 7.5529 - val_loss: 11.5870\n",
      "Epoch 29/100\n",
      "Epoch 00028: val_loss did not improve\n",
      "0s - loss: 7.0784 - val_loss: 11.9387\n",
      "Epoch 30/100\n",
      "Epoch 00029: val_loss improved from 11.58699 to 11.48889, saving model to best.model\n",
      "0s - loss: 7.2675 - val_loss: 11.4889\n",
      "Epoch 31/100\n",
      "Epoch 00030: val_loss improved from 11.48889 to 10.49720, saving model to best.model\n",
      "0s - loss: 7.3047 - val_loss: 10.4972\n",
      "Epoch 32/100\n",
      "Epoch 00031: val_loss improved from 10.49720 to 7.05098, saving model to best.model\n",
      "0s - loss: 7.4883 - val_loss: 7.0510\n",
      "Epoch 33/100\n",
      "Epoch 00032: val_loss improved from 7.05098 to 5.74040, saving model to best.model\n",
      "0s - loss: 6.6589 - val_loss: 5.7404\n",
      "Epoch 34/100\n",
      "Epoch 00033: val_loss did not improve\n",
      "0s - loss: 6.2219 - val_loss: 5.8928\n",
      "Epoch 35/100\n",
      "Epoch 00034: val_loss improved from 5.74040 to 5.57881, saving model to best.model\n",
      "0s - loss: 6.0818 - val_loss: 5.5788\n",
      "Epoch 36/100\n",
      "Epoch 00035: val_loss improved from 5.57881 to 4.83675, saving model to best.model\n",
      "0s - loss: 6.4966 - val_loss: 4.8367\n",
      "Epoch 37/100\n",
      "Epoch 00036: val_loss improved from 4.83675 to 4.38014, saving model to best.model\n",
      "0s - loss: 6.8456 - val_loss: 4.3801\n",
      "Epoch 38/100\n",
      "Epoch 00037: val_loss did not improve\n",
      "0s - loss: 6.2443 - val_loss: 5.0997\n",
      "Epoch 39/100\n",
      "Epoch 00038: val_loss did not improve\n",
      "0s - loss: 7.1090 - val_loss: 5.1730\n",
      "Epoch 40/100\n",
      "Epoch 00039: val_loss improved from 4.38014 to 4.13112, saving model to best.model\n",
      "0s - loss: 6.2597 - val_loss: 4.1311\n",
      "Epoch 41/100\n",
      "Epoch 00040: val_loss improved from 4.13112 to 3.24659, saving model to best.model\n",
      "0s - loss: 6.9416 - val_loss: 3.2466\n",
      "Epoch 42/100\n",
      "Epoch 00041: val_loss improved from 3.24659 to 2.88153, saving model to best.model\n",
      "0s - loss: 6.3900 - val_loss: 2.8815\n",
      "Epoch 43/100\n",
      "Epoch 00042: val_loss improved from 2.88153 to 2.50311, saving model to best.model\n",
      "0s - loss: 6.2622 - val_loss: 2.5031\n",
      "Epoch 44/100\n",
      "Epoch 00043: val_loss did not improve\n",
      "0s - loss: 6.2314 - val_loss: 2.6958\n",
      "Epoch 45/100\n",
      "Epoch 00044: val_loss did not improve\n",
      "0s - loss: 6.4613 - val_loss: 2.5901\n",
      "Epoch 46/100\n",
      "Epoch 00045: val_loss improved from 2.50311 to 2.48320, saving model to best.model\n",
      "0s - loss: 6.0445 - val_loss: 2.4832\n",
      "Epoch 47/100\n",
      "Epoch 00046: val_loss did not improve\n",
      "0s - loss: 6.2207 - val_loss: 3.6255\n",
      "Epoch 48/100\n",
      "Epoch 00047: val_loss did not improve\n",
      "0s - loss: 6.8378 - val_loss: 3.6502\n",
      "Epoch 49/100\n",
      "Epoch 00048: val_loss did not improve\n",
      "0s - loss: 5.7605 - val_loss: 2.7111\n",
      "Epoch 50/100\n",
      "Epoch 00049: val_loss improved from 2.48320 to 2.46271, saving model to best.model\n",
      "0s - loss: 6.3942 - val_loss: 2.4627\n",
      "Epoch 51/100\n",
      "Epoch 00050: val_loss improved from 2.46271 to 2.06290, saving model to best.model\n",
      "0s - loss: 6.3046 - val_loss: 2.0629\n",
      "Epoch 52/100\n",
      "Epoch 00051: val_loss did not improve\n",
      "0s - loss: 6.1813 - val_loss: 2.0805\n",
      "Epoch 53/100\n",
      "Epoch 00052: val_loss did not improve\n",
      "0s - loss: 5.9473 - val_loss: 2.3497\n",
      "Epoch 54/100\n",
      "Epoch 00053: val_loss did not improve\n",
      "0s - loss: 5.9091 - val_loss: 2.2078\n",
      "Epoch 55/100\n",
      "Epoch 00054: val_loss improved from 2.06290 to 1.62532, saving model to best.model\n",
      "0s - loss: 6.3305 - val_loss: 1.6253\n",
      "Epoch 56/100\n",
      "Epoch 00055: val_loss improved from 1.62532 to 1.36467, saving model to best.model\n",
      "0s - loss: 5.5303 - val_loss: 1.3647\n",
      "Epoch 57/100\n",
      "Epoch 00056: val_loss improved from 1.36467 to 1.20125, saving model to best.model\n",
      "0s - loss: 5.3326 - val_loss: 1.2012\n",
      "Epoch 58/100\n",
      "Epoch 00057: val_loss improved from 1.20125 to 0.98550, saving model to best.model\n",
      "0s - loss: 5.8865 - val_loss: 0.9855\n",
      "Epoch 59/100\n",
      "Epoch 00058: val_loss improved from 0.98550 to 0.97536, saving model to best.model\n",
      "0s - loss: 5.5315 - val_loss: 0.9754\n",
      "Epoch 60/100\n",
      "Epoch 00059: val_loss improved from 0.97536 to 0.79695, saving model to best.model\n",
      "0s - loss: 5.7219 - val_loss: 0.7969\n",
      "Epoch 61/100\n",
      "Epoch 00060: val_loss improved from 0.79695 to 0.79548, saving model to best.model\n",
      "0s - loss: 5.7839 - val_loss: 0.7955\n",
      "Epoch 62/100\n",
      "Epoch 00061: val_loss improved from 0.79548 to 0.70505, saving model to best.model\n",
      "0s - loss: 5.3319 - val_loss: 0.7051\n",
      "Epoch 63/100\n",
      "Epoch 00062: val_loss improved from 0.70505 to 0.47554, saving model to best.model\n",
      "0s - loss: 5.9156 - val_loss: 0.4755\n",
      "Epoch 64/100\n",
      "Epoch 00063: val_loss did not improve\n",
      "0s - loss: 5.6898 - val_loss: 0.5246\n",
      "Epoch 65/100\n",
      "Epoch 00064: val_loss did not improve\n",
      "0s - loss: 5.7928 - val_loss: 1.4767\n",
      "Epoch 66/100\n",
      "Epoch 00065: val_loss did not improve\n",
      "0s - loss: 5.4853 - val_loss: 1.0746\n",
      "Epoch 67/100\n",
      "Epoch 00066: val_loss did not improve\n",
      "0s - loss: 5.0537 - val_loss: 0.5905\n",
      "Epoch 68/100\n",
      "Epoch 00067: val_loss did not improve\n",
      "0s - loss: 5.3442 - val_loss: 0.7156\n",
      "Epoch 69/100\n",
      "Epoch 00068: val_loss did not improve\n",
      "0s - loss: 5.3303 - val_loss: 1.1682\n",
      "Epoch 70/100\n",
      "Epoch 00069: val_loss did not improve\n",
      "0s - loss: 5.4408 - val_loss: 1.2726\n",
      "Epoch 71/100\n",
      "Epoch 00070: val_loss did not improve\n",
      "0s - loss: 4.9987 - val_loss: 0.9961\n",
      "Epoch 72/100\n",
      "Epoch 00071: val_loss did not improve\n",
      "0s - loss: 5.1649 - val_loss: 1.2228\n",
      "Epoch 73/100\n",
      "Epoch 00072: val_loss did not improve\n",
      "0s - loss: 5.2992 - val_loss: 1.0324\n",
      "Epoch 74/100\n",
      "Epoch 00073: val_loss did not improve\n",
      "0s - loss: 5.3475 - val_loss: 0.7235\n",
      "Epoch 75/100\n",
      "Epoch 00074: val_loss did not improve\n",
      "0s - loss: 4.8097 - val_loss: 0.6937\n",
      "Epoch 76/100\n",
      "Epoch 00075: val_loss did not improve\n",
      "0s - loss: 5.3388 - val_loss: 0.8046\n",
      "Epoch 77/100\n",
      "Epoch 00076: val_loss did not improve\n",
      "0s - loss: 5.4178 - val_loss: 0.5033\n",
      "Epoch 78/100\n",
      "Epoch 00077: val_loss improved from 0.47554 to 0.42523, saving model to best.model\n",
      "0s - loss: 4.7477 - val_loss: 0.4252\n",
      "Epoch 79/100\n",
      "Epoch 00078: val_loss improved from 0.42523 to 0.42349, saving model to best.model\n",
      "0s - loss: 4.9835 - val_loss: 0.4235\n",
      "Epoch 80/100\n",
      "Epoch 00079: val_loss improved from 0.42349 to 0.28481, saving model to best.model\n",
      "0s - loss: 5.2423 - val_loss: 0.2848\n",
      "Epoch 81/100\n",
      "Epoch 00080: val_loss did not improve\n",
      "0s - loss: 5.1097 - val_loss: 0.2911\n",
      "Epoch 82/100\n",
      "Epoch 00081: val_loss did not improve\n",
      "0s - loss: 5.3153 - val_loss: 0.3702\n",
      "Epoch 83/100\n",
      "Epoch 00082: val_loss did not improve\n",
      "0s - loss: 4.8244 - val_loss: 0.4896\n",
      "Epoch 84/100\n",
      "Epoch 00083: val_loss did not improve\n",
      "0s - loss: 5.3512 - val_loss: 0.5766\n",
      "Epoch 85/100\n",
      "Epoch 00084: val_loss did not improve\n",
      "0s - loss: 5.2602 - val_loss: 0.4992\n",
      "Epoch 86/100\n",
      "Epoch 00085: val_loss did not improve\n",
      "0s - loss: 5.1013 - val_loss: 0.4861\n",
      "Epoch 87/100\n",
      "Epoch 00086: val_loss did not improve\n",
      "0s - loss: 4.5154 - val_loss: 0.5385\n",
      "Epoch 88/100\n",
      "Epoch 00087: val_loss did not improve\n",
      "0s - loss: 4.9169 - val_loss: 0.5602\n",
      "Epoch 89/100\n",
      "Epoch 00088: val_loss did not improve\n",
      "0s - loss: 4.6795 - val_loss: 0.5624\n",
      "Epoch 90/100\n",
      "Epoch 00089: val_loss did not improve\n",
      "0s - loss: 4.7559 - val_loss: 0.5325\n",
      "Epoch 91/100\n",
      "Epoch 00090: val_loss did not improve\n",
      "0s - loss: 4.7968 - val_loss: 0.5918\n",
      "Epoch 92/100\n",
      "Epoch 00091: val_loss did not improve\n",
      "0s - loss: 5.0307 - val_loss: 0.5776\n",
      "Epoch 93/100\n",
      "Epoch 00092: val_loss did not improve\n",
      "0s - loss: 4.9331 - val_loss: 0.6111\n",
      "Epoch 94/100\n",
      "Epoch 00093: val_loss did not improve\n",
      "0s - loss: 4.3268 - val_loss: 0.6095\n",
      "Epoch 95/100\n",
      "Epoch 00094: val_loss did not improve\n",
      "0s - loss: 4.6686 - val_loss: 0.5483\n",
      "Epoch 96/100\n",
      "Epoch 00095: val_loss did not improve\n",
      "0s - loss: 5.2410 - val_loss: 0.5050\n",
      "Epoch 97/100\n",
      "Epoch 00096: val_loss did not improve\n",
      "0s - loss: 4.7337 - val_loss: 0.5747\n",
      "Epoch 98/100\n",
      "Epoch 00097: val_loss did not improve\n",
      "0s - loss: 4.6748 - val_loss: 0.5010\n",
      "Epoch 99/100\n",
      "Epoch 00098: val_loss did not improve\n",
      "0s - loss: 4.7401 - val_loss: 0.5281\n",
      "Epoch 100/100\n",
      "Epoch 00099: val_loss did not improve\n",
      "0s - loss: 4.7383 - val_loss: 0.4886\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "dropout = 0.5\n",
    "init_mean = 0.0\n",
    "init_stdev = 0.05\n",
    "\n",
    "m = Sequential()\n",
    "m.add(Dense(1024, input_shape=(X.shape[1],)\n",
    "            ,kernel_initializer=initializers.TruncatedNormal(mean=init_mean\n",
    "                                                             ,stddev=init_stdev\n",
    "                                                             ,seed=seed)\n",
    "            ,bias_initializer='zeros'))\n",
    "m.add(BatchNormalization())\n",
    "m.add(LeakyReLU())  # helps to stop disappearing gradient\n",
    "m.add(Dropout(dropout))\n",
    "\n",
    "m.add(Dense(512\n",
    "            ,kernel_initializer=initializers.TruncatedNormal(mean=init_mean\n",
    "                                                             ,stddev=init_stdev\n",
    "                                                             ,seed=seed)\n",
    "            ,bias_initializer='zeros'))\n",
    "m.add(BatchNormalization())\n",
    "m.add(LeakyReLU())\n",
    "m.add(Dropout(dropout))\n",
    "\n",
    "m.add(Dense(128\n",
    "             ,kernel_initializer=initializers.TruncatedNormal(mean=init_mean\n",
    "                                                             ,stddev=init_stdev\n",
    "                                                             ,seed=seed)\n",
    "            ,bias_initializer='zeros'))\n",
    "m.add(BatchNormalization())\n",
    "m.add(LeakyReLU())\n",
    "m.add(Dropout(dropout))\n",
    "\n",
    "m.add(Dense(1, activation=None))  # linear activation for regression\n",
    "\n",
    "m.compile(\n",
    "    optimizer=optimizers.Adam(lr=0.002),\n",
    "    loss='mean_squared_error',\n",
    "    #metrics=[log_rmse]\n",
    ")\n",
    "\n",
    "print('Done')\n",
    "\n",
    "epochs = 100\n",
    "batch_size= 256\n",
    "\n",
    "print('Starting training....')\n",
    "\n",
    "m.fit(\n",
    "    # Feature matrix\n",
    "    X_train,\n",
    "    # Target class one-hot-encoded\n",
    "    y_train,\n",
    "    # Iterations to be run if not stopped by EarlyStopping\n",
    "    epochs=epochs,\n",
    "    callbacks=[\n",
    "        # Stop iterations when validation loss has not improved\n",
    "        EarlyStopping(monitor='val_loss', patience=25),\n",
    "        # Nice for keeping the last model before overfitting occurs\n",
    "        ModelCheckpoint(\n",
    "            'best.model',\n",
    "            monitor='val_loss',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        )\n",
    "    ],\n",
    "    verbose=2,\n",
    "    validation_split=0.1,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "print('Done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Now for the Submission set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "df = (\n",
    "    pd.read_csv('input/test.csv') # change this to run on kaggle\n",
    "    #pd.read_csv('../input/train.csv')\n",
    "\n",
    "    # Rename columns to lowercase and underscores\n",
    "    .pipe(lambda d: d.rename(columns={\n",
    "        k: v for k, v in zip(\n",
    "            d.columns,\n",
    "            [c.lower().replace(' ', '_') for c in d.columns]\n",
    "        )\n",
    "    }))\n",
    "    # Switch categorical classes to integers\n",
    "    #.assign(**{target_variable: lambda r: r[target_variable].astype('category').cat.codes})\n",
    ")\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# make the dummy columns for categoricals\n",
    "encoded_data, _ = number_encode_features(df)\n",
    "\n",
    "X_sub = (# Min-max-scaling (only needed for the DL model)\n",
    "    encoded_data.pipe(lambda d: (d-d.min())/d.max()).fillna(0)\n",
    "    .as_matrix()\n",
    ")\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "_execution_state": "idle",
    "_uuid": "d3bcb2c810a18d62374de514e05fbd90a80e894e",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a sample...\n",
      "     Id      SalePrice\n",
      "0  1461  146040.984375\n",
      "1  1462  226797.500000\n",
      "2  1463  175905.562500\n",
      "3  1464  238224.109375\n",
      "4  1465  201734.234375\n",
      "5  1466  232493.140625\n",
      "6  1467  230393.218750\n",
      "7  1468  215095.250000\n",
      "8  1469  262079.515625\n",
      "9  1470  122354.031250\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "## Save to CSV with Image name and results\n",
    "# Run the model\n",
    "y_sub_preds = np.exp( m.predict(X_sub) ) # bring them back to sales prices using exponential\n",
    "pred = pd.DataFrame(data=y_sub_preds) \n",
    "\n",
    "print(\"Here is a sample...\")\n",
    "\n",
    "result = pd.concat([df['id'], pred], axis=1)\n",
    "result.columns = ['Id','SalePrice'] \n",
    "print(result[0:10])\n",
    "\n",
    "# Header: [image ALB BET DOL LAG NoF OTHER   SHARK   YFT]\n",
    "result.to_csv('submission.csv', index = False)\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_execution_state": "idle",
    "_uuid": "d423ee090866fa833500dc505792a781b1f11dcf",
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_execution_state": "idle",
    "_uuid": "a570fcb5b113c65df0726b8566395b95c19e544f",
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
